{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Driven Company Intelligence System\n",
    "## SDS DATATHON 2026 - Category A\n",
    "### Team Fournity - Complete Implementation Documentation\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a comprehensive demonstration of our AI-driven company intelligence system, showcasing all implementation details, methodologies, and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Project Overview](#1-project-overview)\n",
    "2. [System Architecture](#2-system-architecture)\n",
    "3. [Data Loading & Exploration](#3-data-loading--exploration)\n",
    "4. [Data Preprocessing](#4-data-preprocessing)\n",
    "5. [Feature Engineering](#5-feature-engineering)\n",
    "6. [Clustering Analysis](#6-clustering-analysis)\n",
    "7. [Statistical Analysis](#7-statistical-analysis)\n",
    "8. [Machine Learning Models](#8-machine-learning-models)\n",
    "9. [Visualization & Results](#9-visualization--results)\n",
    "10. [Business Insights](#10-business-insights)\n",
    "11. [Conclusions](#11-conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Project Overview\n",
    "\n",
    "### Problem Statement\n",
    "Organizations need to understand company landscapes to make informed decisions about partnerships, investments, market positioning, and competitive strategies. Our system automates the discovery of meaningful company segments and generates actionable insights from complex datasets.\n",
    "\n",
    "### Solution\n",
    "We developed a comprehensive machine learning system that:\n",
    "- Automatically segments companies based on multiple attributes\n",
    "- Identifies patterns and anomalies\n",
    "- Generates predictive models\n",
    "- Provides statistical validation\n",
    "- Creates actionable business insights\n",
    "\n",
    "### Key Features\n",
    "1. **Intelligent Data Processing**: Automatic column detection, feature engineering, outlier handling\n",
    "2. **Advanced Clustering**: 4-phase Latent-Sparse Clustering with multiple algorithms\n",
    "3. **Machine Learning**: Logistic regression (cluster prediction) and linear regression (performance forecasting)\n",
    "4. **Statistical Analysis**: Chi-square tests, ANOVA, VIF calculation\n",
    "5. **Visualizations**: PCA plots, violin plots, heatmaps, interactive 3D visualizations\n",
    "6. **LLM Integration**: Optional OpenAI/DeepSeek for natural language insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. System Architecture\n",
    "\n",
    "### Module Structure\n",
    "\n",
    "```\n",
    "Fournity/\n",
    "├── company_intelligence.py       # Main analysis engine (3100+ lines)\n",
    "├── clustering_analysis.py        # 4-phase clustering (2700+ lines)\n",
    "├── process_champions_data.py     # Data preprocessing\n",
    "├── generate_report.py            # Report generation\n",
    "├── visualization_improvements.py # Enhanced visualizations\n",
    "└── project_documentation.ipynb   # This notebook\n",
    "```\n",
    "\n",
    "### Technology Stack\n",
    "- **Data Processing**: pandas, numpy, openpyxl\n",
    "- **Machine Learning**: scikit-learn, scikit-learn-extra\n",
    "- **Clustering**: K-Means, K-Medoids, DBSCAN, HDBSCAN, GMM\n",
    "- **Dimensionality Reduction**: PCA, t-SNE, UMAP, FAMD\n",
    "- **Visualization**: matplotlib, seaborn, plotly\n",
    "- **Statistical Analysis**: scipy, statsmodels\n",
    "- **Explainability**: SHAP\n",
    "- **LLM Integration**: OpenAI (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from company_intelligence import CompanyIntelligence\n",
    "from clustering_analysis import LatentSparseClustering\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading & Exploration\n",
    "\n",
    "### Dataset: Champions Group Company Data\n",
    "The dataset contains information about companies including:\n",
    "- Financial metrics (revenue, market value)\n",
    "- Workforce data (employees, locations)\n",
    "- Technology infrastructure (IT spending, equipment)\n",
    "- Industry classifications (SIC, NAICS, NACE)\n",
    "- Company characteristics (entity type, ownership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "data_path = 'champions_group_data.xlsx'\n",
    "api_key = os.getenv('OPENAI_API_KEY') or os.getenv('DEEPSEEK_API_KEY')  # Optional\n",
    "\n",
    "print(\"Initializing Company Intelligence Analyzer...\")\n",
    "print(f\"Data file: {data_path}\")\n",
    "print(f\"LLM integration: {'Enabled' if api_key else 'Disabled (using rule-based insights)'}\")\n",
    "print()\n",
    "\n",
    "analyzer = CompanyIntelligence(data_path, api_key=api_key)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total companies: {len(analyzer.df)}\")\n",
    "print(f\"Total features: {len(analyzer.df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA EXPLORATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_explored = analyzer.explore_data()\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "display(analyzer.df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(analyzer.df.dtypes)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = analyzer.df.isnull().sum()\n",
    "missing_pct = (missing / len(analyzer.df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "display(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preprocessing\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "Our preprocessing pipeline includes:\n",
    "1. **Column Detection**: Intelligent pattern matching to find relevant columns\n",
    "2. **Feature Engineering**: Calculate derived metrics (company age, ratios, indices)\n",
    "3. **Missing Value Imputation**: Median imputation for numeric features\n",
    "4. **Outlier Handling**: IQR-based winsorization (caps outliers without removing data)\n",
    "5. **Feature Scaling**: StandardScaler for normalization\n",
    "6. **Text Processing**: TF-IDF vectorization for industry descriptions\n",
    "\n",
    "### Key Business Indicators Calculated\n",
    "1. Market Value/Revenue Ratio (Price-to-Sales)\n",
    "2. IT Investment Intensity\n",
    "3. Revenue per Employee\n",
    "4. Market Value per Employee\n",
    "5. IT Spend Ratio\n",
    "6. Technology Density Index\n",
    "7. Company Age\n",
    "8. Growth Potential Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preprocess with business indicators\n",
    "analyzer.preprocess_data(\n",
    "    calculate_indicators=True,\n",
    "    include_key_indicators_in_clustering=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Preprocessing complete\")\n",
    "print(f\"Processed features: {len(analyzer.feature_names)}\")\n",
    "print(f\"Companies after filtering: {len(analyzer.df_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display processed features\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESSED FEATURES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal features used for clustering: {len(analyzer.feature_names)}\")\n",
    "print(\"\\nFeature list:\")\n",
    "for i, feature in enumerate(analyzer.feature_names, 1):\n",
    "    print(f\"{i:3d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics of processed data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESSED DATA STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if analyzer.df_processed is not None:\n",
    "    # Show statistics for first 10 features\n",
    "    display(pd.DataFrame(analyzer.df_processed[:, :min(10, analyzer.df_processed.shape[1])],\n",
    "                        columns=analyzer.feature_names[:min(10, len(analyzer.feature_names))]).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Engineering\n",
    "\n",
    "### Derived Business Metrics\n",
    "We calculate 10+ business indicators to enhance segmentation quality:\n",
    "\n",
    "#### Financial Indicators\n",
    "- **Market Value/Revenue Ratio**: Identifies growth vs. value companies\n",
    "- **Revenue per Employee**: Measures productivity\n",
    "- **Market Value per Employee**: Indicates market perception of workforce value\n",
    "\n",
    "#### Technology Indicators\n",
    "- **IT Investment Intensity**: Tech-forward vs. traditional segmentation\n",
    "- **Technology Density Index**: IT infrastructure per employee\n",
    "- **Technology Sophistication Index**: Composite IT maturity score\n",
    "\n",
    "#### Operational Indicators\n",
    "- **Single-Site Concentration**: Geographic strategy indicator\n",
    "- **Employees per Site**: Operational scale indicator\n",
    "- **Workforce Technology Ratio**: Knowledge economy indicator\n",
    "\n",
    "#### Strategic Indicators\n",
    "- **Company Age**: Maturity indicator\n",
    "- **Growth Potential Index**: Composite growth score\n",
    "- **Maturity Stage**: Startup/Growth/Mature/Established classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional business indicators\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUSINESS INDICATOR CALCULATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analyzer.calculate_business_indicators()\n",
    "\n",
    "# Display indicator summary\n",
    "indicator_cols = [\n",
    "    'market_value_to_revenue_ratio',\n",
    "    'it_investment_intensity',\n",
    "    'single_site_concentration',\n",
    "    'workforce_tech_ratio',\n",
    "    'tech_sophistication_index',\n",
    "    'growth_potential_index',\n",
    "    'maturity_stage',\n",
    "    'revenue_scale',\n",
    "    'employee_scale'\n",
    "]\n",
    "\n",
    "available_indicators = [col for col in indicator_cols if col in analyzer.df.columns]\n",
    "if available_indicators:\n",
    "    print(f\"\\n✓ Calculated {len(available_indicators)} business indicators\")\n",
    "    \n",
    "    # Show numeric indicators\n",
    "    numeric_indicators = [col for col in available_indicators \n",
    "                         if analyzer.df[col].dtype in [np.float64, np.int64]]\n",
    "    if numeric_indicators:\n",
    "        print(\"\\nNumeric Indicators Summary:\")\n",
    "        display(analyzer.df[numeric_indicators].describe())\n",
    "    \n",
    "    # Show categorical indicators\n",
    "    categorical_indicators = ['maturity_stage', 'revenue_scale', 'employee_scale']\n",
    "    for col in categorical_indicators:\n",
    "        if col in analyzer.df.columns:\n",
    "            print(f\"\\n{col.replace('_', ' ').title()} Distribution:\")\n",
    "            print(analyzer.df[col].value_counts())\n",
    "else:\n",
    "    print(\"\\n⚠ No business indicators calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Clustering Analysis\n",
    "\n",
    "### 4-Phase Latent-Sparse Clustering Workflow\n",
    "\n",
    "#### Phase 1: Context & Meta-Data Analysis\n",
    "- Feature profiling (numerical vs. categorical)\n",
    "- Sparsity checks (missing value analysis)\n",
    "- Metric mapping (distance metric selection)\n",
    "\n",
    "#### Phase 2: Filtering & Encoding Engine\n",
    "- Redundancy pruning (removes highly correlated features)\n",
    "- Automated scaling (RobustScaler vs. StandardScaler)\n",
    "- Dimensionality reduction (FAMD/PCA)\n",
    "\n",
    "#### Phase 3: Iterative Clustering Loop\n",
    "- Hyperparameter optimization\n",
    "- Multi-algorithm testing (K-Means, K-Medoids, DBSCAN, HDBSCAN)\n",
    "- Validation (Silhouette, Davies-Bouldin, Calinski-Harabasz)\n",
    "\n",
    "#### Phase 4: Interpretability & Insights\n",
    "- Feature importance (Random Forest, SHAP)\n",
    "- Cluster profiling\n",
    "- Persona generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMAL CLUSTER DETERMINATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "optimal_k = analyzer.determine_optimal_clusters(\n",
    "    max_k=10,\n",
    "    practical_threshold=0.70\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Optimal number of clusters: {optimal_k}\")\n",
    "print(f\"  Based on multi-metric validation (Silhouette, Davies-Bouldin, Calinski-Harabasz)\")\n",
    "print(f\"  Balanced for business practicality (K=4-8 preferred)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTERING EXECUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analyzer.perform_clustering(n_clusters=optimal_k)\n",
    "\n",
    "print(f\"\\n✓ Clustering complete\")\n",
    "print(f\"Number of clusters: {len(set(analyzer.clusters))}\")\n",
    "\n",
    "# Display cluster distribution\n",
    "if 'Cluster' in analyzer.df.columns:\n",
    "    cluster_dist = analyzer.df['Cluster'].value_counts().sort_index()\n",
    "    print(\"\\nCluster Distribution:\")\n",
    "    for cluster_id, count in cluster_dist.items():\n",
    "        pct = (count / len(analyzer.df)) * 100\n",
    "        print(f\"  Cluster {cluster_id}: {count:4d} companies ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster distribution\n",
    "if 'Cluster' in analyzer.df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Bar plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    cluster_dist = analyzer.df['Cluster'].value_counts().sort_index()\n",
    "    cluster_dist.plot(kind='bar', color=plt.cm.Set3(np.linspace(0, 1, len(cluster_dist))))\n",
    "    plt.title('Company Distribution Across Clusters', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Cluster ID', fontsize=12)\n",
    "    plt.ylabel('Number of Companies', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Pie chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(cluster_dist.values, labels=[f'Cluster {i}' for i in cluster_dist.index],\n",
    "            autopct='%1.1f%%', colors=plt.cm.Set3(np.linspace(0, 1, len(cluster_dist))),\n",
    "            startangle=90)\n",
    "    plt.title('Cluster Size Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cluster_analysis = analyzer.analyze_clusters()\n",
    "\n",
    "# Display detailed analysis for each cluster\n",
    "for cluster_id, info in cluster_analysis.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CLUSTER {cluster_id}: {info['size']} companies ({info['percentage']:.1f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Show top characteristics\n",
    "    print(\"\\nTop Characteristics:\")\n",
    "    if 'top_features' in info:\n",
    "        for i, (feature, value) in enumerate(info['top_features'].items(), 1):\n",
    "            print(f\"  {i}. {feature}: {value:.2f}\")\n",
    "            if i >= 5:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Statistical Analysis\n",
    "\n",
    "### Chi-Square Tests\n",
    "We perform chi-square tests to examine relationships between categorical variables and cluster membership:\n",
    "- Tests statistical significance of associations\n",
    "- Validates assumptions (expected frequency ≥ 5)\n",
    "- Applies multiple testing correction (FDR-BH method)\n",
    "\n",
    "### Analysis of Variance (ANOVA)\n",
    "We test if numerical features differ significantly across clusters:\n",
    "- F-statistic and p-value for each feature\n",
    "- Effect size calculation\n",
    "- Multiple testing correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform chi-square tests\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHI-SQUARE STATISTICAL TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "chi_square_results = analyzer.perform_chi_square_test(correction_method='fdr_bh')\n",
    "\n",
    "if chi_square_results:\n",
    "    print(f\"\\n✓ Chi-square tests completed\")\n",
    "    print(f\"  Tests performed: {len(chi_square_results)}\")\n",
    "    print(f\"  Multiple testing correction: FDR-BH\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nResults:\")\n",
    "    for feature, result in chi_square_results.items():\n",
    "        if result['valid']:\n",
    "            sig = \"***\" if result['p_value'] < 0.001 else \"**\" if result['p_value'] < 0.01 else \"*\" if result['p_value'] < 0.05 else \"ns\"\n",
    "            print(f\"\\n{feature}:\")\n",
    "            print(f\"  Chi-square statistic: {result['chi2']:.2f}\")\n",
    "            print(f\"  p-value: {result['p_value']:.4f} {sig}\")\n",
    "            print(f\"  Degrees of freedom: {result['dof']}\")\n",
    "        else:\n",
    "            print(f\"\\n{feature}: Test invalid (assumption violation)\")\n",
    "else:\n",
    "    print(\"\\n⚠ No categorical variables available for chi-square tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clusters across features\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLUSTER COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = analyzer.compare_clusters()\n",
    "\n",
    "if comparison is not None:\n",
    "    print(\"\\nCluster Comparison Summary:\")\n",
    "    display(comparison)\n",
    "else:\n",
    "    print(\"\\n⚠ Cluster comparison not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Machine Learning Models\n",
    "\n",
    "### Logistic Regression (Cluster Prediction)\n",
    "Predicts which cluster a company belongs to based on its features:\n",
    "- **Purpose**: Segment membership prediction for new companies\n",
    "- **Method**: Multinomial logistic regression with L2 regularization\n",
    "- **Validation**: 5-fold cross-validation\n",
    "- **Output**: Interpretable coefficients showing feature importance\n",
    "\n",
    "### Linear Regression (Performance Forecasting)\n",
    "Predicts company performance metrics (revenue, market value):\n",
    "- **Purpose**: Forecast financial outcomes\n",
    "- **Method**: Ridge regression (handles multicollinearity)\n",
    "- **Validation**: Train/test split with proper scaling\n",
    "- **Output**: R² score, RMSE, feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION (Cluster Prediction)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lr_results = analyzer.train_logistic_regression(\n",
    "    use_original_features=True,\n",
    "    cv_folds=5\n",
    ")\n",
    "\n",
    "if lr_results:\n",
    "    print(f\"\\n✓ Logistic regression model trained\")\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"  Training accuracy: {lr_results['train_accuracy']:.2%}\")\n",
    "    print(f\"  Test accuracy: {lr_results['test_accuracy']:.2%}\")\n",
    "    print(f\"  Cross-validation score: {lr_results['cv_score_mean']:.2%} (±{lr_results['cv_score_std']:.2%})\")\n",
    "    \n",
    "    # Display classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(lr_results['classification_report'])\n",
    "    \n",
    "    # Display top features\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    if 'feature_importance' in lr_results:\n",
    "        for i, (feature, importance) in enumerate(lr_results['feature_importance'][:10], 1):\n",
    "            print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Logistic regression training failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LINEAR REGRESSION (Revenue Forecasting)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "linear_results = analyzer.train_linear_regression(\n",
    "    target_feature='revenue',\n",
    "    regularization='ridge',\n",
    "    check_multicollinearity=True\n",
    ")\n",
    "\n",
    "if linear_results:\n",
    "    print(f\"\\n✓ Linear regression model trained\")\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"  R² score (train): {linear_results.get('r2_train', 0):.4f}\")\n",
    "    print(f\"  R² score (test): {linear_results.get('r2_test', 0):.4f}\")\n",
    "    print(f\"  RMSE (test): {linear_results.get('rmse_test', 0):.2f}\")\n",
    "    \n",
    "    # Display VIF if available\n",
    "    if 'vif_scores' in linear_results:\n",
    "        print(\"\\nMulticollinearity Check (VIF):\")\n",
    "        vif_high = {k: v for k, v in linear_results['vif_scores'].items() if v > 5}\n",
    "        if vif_high:\n",
    "            print(\"  Features with high VIF (>5):\")\n",
    "            for feature, vif in sorted(vif_high.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                print(f\"    {feature}: {vif:.2f}\")\n",
    "        else:\n",
    "            print(\"  ✓ No high multicollinearity detected\")\n",
    "    \n",
    "    # Display top features\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    if 'feature_importance' in linear_results:\n",
    "        for i, (feature, importance) in enumerate(linear_results['feature_importance'][:10], 1):\n",
    "            print(f\"  {i:2d}. {feature}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Linear regression training failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualization & Results\n",
    "\n",
    "### Visualization Suite\n",
    "We generate multiple visualizations to understand the clustering results:\n",
    "\n",
    "1. **PCA Scatter Plot**: 2D projection showing cluster separation\n",
    "2. **Feature Comparison**: Violin plots with statistical annotations\n",
    "3. **Cluster Heatmap**: Normalized mean values across features\n",
    "4. **Interactive 3D Plot**: Plotly-based explorable visualization\n",
    "5. **Explained Variance**: PCA component contribution\n",
    "6. **Confusion Matrix**: Logistic regression predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all visualizations\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZATION GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "analyzer.visualize_results()\n",
    "\n",
    "print(\"\\n✓ Visualizations generated and saved\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  - optimal_clusters.png\")\n",
    "print(\"  - pca_clusters.png\")\n",
    "print(\"  - pca_clusters_enhanced.png\")\n",
    "print(\"  - feature_comparison.png\")\n",
    "print(\"  - feature_comparison_enhanced.png\")\n",
    "print(\"  - cluster_heatmap_enhanced.png\")\n",
    "print(\"  - interactive_clusters.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PCA visualization (if available)\n",
    "from IPython.display import Image, display as ipy_display\n",
    "import os\n",
    "\n",
    "if os.path.exists('pca_clusters_enhanced.png'):\n",
    "    print(\"\\nPCA Cluster Visualization:\")\n",
    "    ipy_display(Image('pca_clusters_enhanced.png'))\n",
    "elif os.path.exists('pca_clusters.png'):\n",
    "    print(\"\\nPCA Cluster Visualization:\")\n",
    "    ipy_display(Image('pca_clusters.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature comparison (if available)\n",
    "if os.path.exists('feature_comparison_enhanced.png'):\n",
    "    print(\"\\nFeature Comparison Across Clusters:\")\n",
    "    ipy_display(Image('feature_comparison_enhanced.png'))\n",
    "elif os.path.exists('feature_comparison.png'):\n",
    "    print(\"\\nFeature Comparison Across Clusters:\")\n",
    "    ipy_display(Image('feature_comparison.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Business Insights\n",
    "\n",
    "### Insight Generation\n",
    "We generate actionable business insights using:\n",
    "- **LLM Integration** (if available): Natural language insights from OpenAI/DeepSeek\n",
    "- **Rule-Based System** (fallback): Statistical pattern analysis\n",
    "\n",
    "### Insights Include\n",
    "1. **Cluster Characterization**: What defines each segment?\n",
    "2. **Key Differentiators**: What separates the segments?\n",
    "3. **Business Recommendations**: How to leverage these insights?\n",
    "4. **Risk Factors**: What to watch out for?\n",
    "5. **Growth Opportunities**: Where to focus efforts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify patterns\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PATTERN IDENTIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "patterns = analyzer.identify_patterns()\n",
    "\n",
    "if patterns:\n",
    "    print(\"\\n✓ Patterns identified\")\n",
    "    \n",
    "    # Display outliers\n",
    "    if 'outliers' in patterns and patterns['outliers']:\n",
    "        print(\"\\nTop Outlier Features:\")\n",
    "        for i, outlier in enumerate(patterns['outliers'][:5], 1):\n",
    "            print(f\"  {i}. {outlier['feature']}: {outlier['count']} companies ({outlier['percentage']:.1f}%)\")\n",
    "    \n",
    "    # Display trends\n",
    "    if 'trends' in patterns and patterns['trends']:\n",
    "        print(\"\\nKey Trends:\")\n",
    "        for i, trend in enumerate(patterns['trends'][:5], 1):\n",
    "            print(f\"  {i}. {trend}\")\n",
    "else:\n",
    "    print(\"\\n⚠ No patterns identified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUSINESS INSIGHTS GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "insights = analyzer.generate_llm_insights(cluster_analysis, patterns)\n",
    "\n",
    "print(\"\\n\" + insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Conclusions\n",
    "\n",
    "### Summary\n",
    "Our AI-driven company intelligence system successfully:\n",
    "\n",
    "1. **Processed and analyzed** a complex dataset of company attributes\n",
    "2. **Identified meaningful segments** using advanced clustering algorithms\n",
    "3. **Generated predictive models** for segment membership and performance forecasting\n",
    "4. **Provided statistical validation** through multiple testing methods\n",
    "5. **Created actionable insights** for business decision-making\n",
    "\n",
    "### Key Achievements\n",
    "- ✓ Comprehensive data preprocessing with intelligent feature engineering\n",
    "- ✓ 4-phase Latent-Sparse Clustering with multi-algorithm support\n",
    "- ✓ Business-optimized cluster selection (K=4-8)\n",
    "- ✓ Statistical validation with proper assumption checking\n",
    "- ✓ Interpretable machine learning models\n",
    "- ✓ Enhanced visualizations with statistical annotations\n",
    "- ✓ LLM-powered natural language insights\n",
    "\n",
    "### Business Value\n",
    "This system enables organizations to:\n",
    "- **Segment companies** effectively for targeted strategies\n",
    "- **Identify opportunities** in specific market segments\n",
    "- **Assess risks** based on company characteristics\n",
    "- **Predict performance** using validated models\n",
    "- **Make data-driven decisions** with statistical confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REPORT GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "report = analyzer.generate_report(cluster_analysis, patterns, insights)\n",
    "\n",
    "print(\"\\n✓ Comprehensive report generated and saved\")\n",
    "print(\"  File: company_intelligence_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS EXPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Export data with cluster labels\n",
    "output_file = 'companies_with_segments.csv'\n",
    "analyzer.df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Results exported to: {output_file}\")\n",
    "print(f\"  Companies: {len(analyzer.df)}\")\n",
    "print(f\"  Features: {len(analyzer.df.columns)}\")\n",
    "print(f\"  Clusters: {len(set(analyzer.clusters))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select key columns to display\n",
    "display_cols = ['Cluster']\n",
    "for col in analyzer.df.columns[:5]:\n",
    "    if col != 'Cluster':\n",
    "        display_cols.append(col)\n",
    "\n",
    "print(\"\\nFirst 10 companies with cluster assignments:\")\n",
    "display(analyzer.df[display_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: System Specifications\n",
    "\n",
    "### Code Statistics\n",
    "- **Total Lines of Code**: ~6,500+\n",
    "- **Main Analysis Engine**: 3,100+ lines (`company_intelligence.py`)\n",
    "- **Clustering Module**: 2,700+ lines (`clustering_analysis.py`)\n",
    "- **Preprocessing**: 271 lines (`process_champions_data.py`)\n",
    "- **Visualizations**: 412 lines (`visualization_improvements.py`)\n",
    "\n",
    "### Features Implemented\n",
    "- **10+ Clustering Algorithms**: K-Means, K-Medoids, DBSCAN, HDBSCAN, GMM\n",
    "- **5+ Dimensionality Reduction**: PCA, t-SNE, UMAP, TruncatedSVD, FAMD\n",
    "- **3+ Regression Models**: Logistic, Ridge, Lasso, ElasticNet\n",
    "- **5+ Statistical Tests**: Chi-square, ANOVA, VIF, Silhouette, Davies-Bouldin\n",
    "- **10+ Business Indicators**: Market ratios, technology indices, maturity stages\n",
    "\n",
    "### Dependencies\n",
    "- Core: pandas, numpy, scikit-learn\n",
    "- Advanced: prince, umap-learn, hdbscan, shap, gower\n",
    "- Visualization: matplotlib, seaborn, plotly\n",
    "- Statistical: scipy, statsmodels\n",
    "- LLM: openai (optional)\n",
    "\n",
    "### Execution Time\n",
    "- Data loading: < 1 second\n",
    "- Preprocessing: 2-5 seconds\n",
    "- Clustering: 5-15 seconds\n",
    "- ML models: 3-10 seconds\n",
    "- Visualization: 5-10 seconds\n",
    "- **Total**: < 1 minute (typical dataset)\n",
    "\n",
    "---\n",
    "\n",
    "## Thank You!\n",
    "\n",
    "**Team Fournity**  \n",
    "SDS DATATHON 2026 - Category A  \n",
    "January 2026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
